{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = pd.read_csv('data/iris.csv')\n",
    "iris = iris.drop(iris.columns[[0]],axis=1)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = {\n",
    "  'setosa': 0,\n",
    "  'versicolor': 1,\n",
    "  'virginica': 2\n",
    " }\n",
    "iris['Species'] = iris['Species'].apply(lambda x: mappings[x])\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.drop('Species', axis=1).values\n",
    "y = iris['Species'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=4, out_features=16)\n",
    "        self.fc2 = nn.Linear(in_features=16, out_features=12)\n",
    "        self.output = nn.Linear(in_features=12, out_features=3)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ANN()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "loss_arr = []\n",
    "for i in range(epochs):\n",
    "    y_hat = model.forward(X_train)\n",
    "    loss = loss(y_hat, y_train)\n",
    "    loss_arr.append(loss)\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f'Epoch: {i} Loss: {loss}')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for val in X_test:\n",
    "        y_hat = model.forward(val)\n",
    "        preds.append(y_hat.argmax().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Y': y_test, 'YHat': preds})\n",
    "df['Correct'] = [1 if corr == pred else 0 for corr, pred in zip(df['Y'], df['YHat'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\AppData\\Local\\Temp\\ipykernel_9932\\1698386021.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  data_x_new[i,:] = data_x_temp[:,i:mem_n+i]\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as scio\n",
    "import numpy as np\n",
    "\n",
    "mem_n = 30;\n",
    "key = 'mt'\n",
    "key1 = key+'_x'\n",
    "dataFile = '../data/data.mat'\n",
    "data = scio.loadmat(dataFile)\n",
    "data.keys()\n",
    "\n",
    "data_x = data[key+'_x']\n",
    "data_y = data[key+'_y']\n",
    "\n",
    "L = len(data_x)\n",
    "\n",
    "data_x_temp = np.pad(data_x,((0, mem_n), (0, 0)), 'wrap').T\n",
    "\n",
    "data_x_new = np.zeros([L,mem_n])\n",
    "for i in range(L):\n",
    "    data_x_new[i,:] = data_x_temp[:,i:mem_n+i]\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = data_x_new[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11.]\n",
      "[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12.]\n",
      "[ 2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "[ 3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14.]\n",
      "[ 4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15.]\n",
      "[ 5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16.]\n",
      "[ 6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.]\n",
      "[ 7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.]\n",
      "[ 8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18. 19.]\n",
      "[ 9. 10. 11. 12. 13. 14. 15. 16. 17. 18. 19. 20.]\n",
      "[10. 11. 12. 13. 14. 15. 16. 17. 18. 19. 20. 21.]\n",
      "[11. 12. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22.]\n",
      "[12. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22. 23.]\n",
      "[13. 14. 15. 16. 17. 18. 19. 20. 21. 22. 23. 24.]\n",
      "[14. 15. 16. 17. 18. 19. 20. 21. 22. 23. 24. 25.]\n",
      "[15. 16. 17. 18. 19. 20. 21. 22. 23. 24. 25. 26.]\n",
      "[16. 17. 18. 19. 20. 21. 22. 23. 24. 25. 26. 27.]\n",
      "[17. 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28.]\n",
      "[18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29.]\n",
      "[19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30.]\n",
      "[20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31.]\n",
      "[21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32.]\n",
      "[22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33.]\n",
      "[23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34.]\n",
      "[24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.]\n",
      "[25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36.]\n",
      "[26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37.]\n",
      "[27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38.]\n",
      "[28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39.]\n",
      "[29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 40.]\n",
      "[30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 40. 41.]\n",
      "[31. 32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 42.]\n",
      "[32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 42. 43.]\n",
      "[33. 34. 35. 36. 37. 38. 39. 40. 41. 42. 43. 44.]\n",
      "[34. 35. 36. 37. 38. 39. 40. 41. 42. 43. 44. 45.]\n",
      "[35. 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46.]\n",
      "[36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47.]\n",
      "[37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48.]\n",
      "[38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49.]\n",
      "[39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50.]\n",
      "[40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51.]\n",
      "[41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52.]\n",
      "[42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53.]\n",
      "[43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53. 54.]\n",
      "[44. 45. 46. 47. 48. 49. 50. 51. 52. 53. 54. 55.]\n",
      "[45. 46. 47. 48. 49. 50. 51. 52. 53. 54. 55. 56.]\n",
      "[46. 47. 48. 49. 50. 51. 52. 53. 54. 55. 56. 57.]\n",
      "[47. 48. 49. 50. 51. 52. 53. 54. 55. 56. 57. 58.]\n",
      "[48. 49. 50. 51. 52. 53. 54. 55. 56. 57. 58. 59.]\n",
      "[49. 50. 51. 52. 53. 54. 55. 56. 57. 58. 59. 60.]\n",
      "[50. 51. 52. 53. 54. 55. 56. 57. 58. 59. 60. 61.]\n",
      "[51. 52. 53. 54. 55. 56. 57. 58. 59. 60. 61. 62.]\n",
      "[52. 53. 54. 55. 56. 57. 58. 59. 60. 61. 62. 63.]\n",
      "[53. 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64.]\n",
      "[54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65.]\n",
      "[55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66.]\n",
      "[56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67.]\n",
      "[57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68.]\n",
      "[58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69.]\n",
      "[59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70.]\n",
      "[60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71.]\n",
      "[61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71. 72.]\n",
      "[62. 63. 64. 65. 66. 67. 68. 69. 70. 71. 72. 73.]\n",
      "[63. 64. 65. 66. 67. 68. 69. 70. 71. 72. 73. 74.]\n",
      "[64. 65. 66. 67. 68. 69. 70. 71. 72. 73. 74. 75.]\n",
      "[65. 66. 67. 68. 69. 70. 71. 72. 73. 74. 75. 76.]\n",
      "[66. 67. 68. 69. 70. 71. 72. 73. 74. 75. 76. 77.]\n",
      "[67. 68. 69. 70. 71. 72. 73. 74. 75. 76. 77. 78.]\n",
      "[68. 69. 70. 71. 72. 73. 74. 75. 76. 77. 78. 79.]\n",
      "[69. 70. 71. 72. 73. 74. 75. 76. 77. 78. 79. 80.]\n",
      "[70. 71. 72. 73. 74. 75. 76. 77. 78. 79. 80. 81.]\n",
      "[71. 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82.]\n",
      "[72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83.]\n",
      "[73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84.]\n",
      "[74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85.]\n",
      "[75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86.]\n",
      "[76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87.]\n",
      "[77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88.]\n",
      "[78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89.]\n",
      "[79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89. 90.]\n",
      "[80. 81. 82. 83. 84. 85. 86. 87. 88. 89. 90. 91.]\n",
      "[81. 82. 83. 84. 85. 86. 87. 88. 89. 90. 91. 92.]\n",
      "[82. 83. 84. 85. 86. 87. 88. 89. 90. 91. 92. 93.]\n",
      "[83. 84. 85. 86. 87. 88. 89. 90. 91. 92. 93. 94.]\n",
      "[84. 85. 86. 87. 88. 89. 90. 91. 92. 93. 94. 95.]\n",
      "[85. 86. 87. 88. 89. 90. 91. 92. 93. 94. 95. 96.]\n",
      "[86. 87. 88. 89. 90. 91. 92. 93. 94. 95. 96. 97.]\n",
      "[87. 88. 89. 90. 91. 92. 93. 94. 95. 96. 97. 98.]\n",
      "[88. 89. 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
      "[89. 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.  0.]\n",
      "[90. 91. 92. 93. 94. 95. 96. 97. 98. 99.  0.  1.]\n",
      "[91. 92. 93. 94. 95. 96. 97. 98. 99.  0.  1.  2.]\n",
      "[92. 93. 94. 95. 96. 97. 98. 99.  0.  1.  2.  3.]\n",
      "[93. 94. 95. 96. 97. 98. 99.  0.  1.  2.  3.  4.]\n",
      "[94. 95. 96. 97. 98. 99.  0.  1.  2.  3.  4.  5.]\n",
      "[95. 96. 97. 98. 99.  0.  1.  2.  3.  4.  5.  6.]\n",
      "[96. 97. 98. 99.  0.  1.  2.  3.  4.  5.  6.  7.]\n",
      "[97. 98. 99.  0.  1.  2.  3.  4.  5.  6.  7.  8.]\n",
      "[98. 99.  0.  1.  2.  3.  4.  5.  6.  7.  8.  9.]\n",
      "[99.  0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.arange(100).reshape(100,1)\n",
    "n = 12\n",
    "b = np.pad(a,((0, n), (0, 0)), 'wrap').T\n",
    "c = np.zeros([100,12])\n",
    "for i in range(100):\n",
    "    c[i,:] = b[:,i:n+i]\n",
    "    print(c[i,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2+2j)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(10)\n",
    "b = np.arange(10)\n",
    "c = a + b*1j\n",
    "c[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
